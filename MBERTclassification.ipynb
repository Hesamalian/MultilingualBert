{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MBERTclassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1fi-QpCyVceEodbZXgoc4PFHjWGEPAypQ",
      "authorship_tag": "ABX9TyN3q43e8F2doEpOEIoWN23Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d4faa29405ab4b13a13ec543aa4516b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8d996e04b4544f03abde0475af800570",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0f3b16c7c4b34024b820988066f686a8",
              "IPY_MODEL_cd5ec585a83344799d50f5d536f488a3"
            ]
          }
        },
        "5058cba4d26543bca499d2b6a4526378": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fce45c6111984e17aa6c9f2e8d2faeea",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_509d5dcd7c944081bc1aaac578cdc311",
              "IPY_MODEL_eb703c8c6b714b8a92ef5dc60bfb8c3d"
            ]
          }
        },
        "b2f97daae4c543adbb1aa00f71692def": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bf154a8c8b284980894547dbc1ab741f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bafb626e2df8447eb37f666e821679c3",
              "IPY_MODEL_9c323dab674643c2bf342917bd0ff9bb"
            ]
          }
        },
        "4d93031e4d0c40559642a8393c6e69f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cdfe2fb7c3e14dd3b297f8f496637176",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6b334f00159d4c26bb12998a0944c72f",
              "IPY_MODEL_b95afaaeff234cac862dcbee718f09f8"
            ]
          }
        },
        "6363711a33ad4c51b4247a7310013ba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f19e5693fa3a46f9865e050ff5a1ce10",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_412a306771cc4260a53c7773da2d6f32",
              "IPY_MODEL_798646122276473d8b6e986c410d66b7"
            ]
          }
        },
        "997013b0fcfd4b76a78b97374494f4fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ccb21a54c9764446887a676db5f845e4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3ba8116b899b4d1985fe8900ddaa101b",
              "IPY_MODEL_adeffd111c19451d9e0ef133b2b5a319"
            ]
          }
        },
        "091c6b8fd2ba46a58f5d5b75caf522ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ce070fa8d9ed4377aab9e9efdce1b4d8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f80a9f1e282a473ca36b0171fd28811e",
              "IPY_MODEL_9eca3b0bb738483a87af5719103debfc"
            ]
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hesamalian/MultilingualBert/blob/master/MBERTclassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5bfGqwmlCe8",
        "colab_type": "text"
      },
      "source": [
        "This is a work to utilize Keras libraries (https://github.com/vzhou842/cnn-from-scratch) and Simple Transformers (https://github.com/ThilinaRajapakse/simpletransformers) to demonstrate the difference between CNN and Multilingual BERT text classifiers.\n",
        "\n",
        "The former one is based on the transformer packages by HuggingFace 🤗 (https://github.com/huggingface/transformers).\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "> This is a task of binary sentiment analysis for a french sample dataset (\"example.csv\") that can be found in https://drive.google.com/open?id=1IyxGimLEytKoAIkbr0wdl_bTdGCuz7uH .\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yngtSzNMAayg",
        "colab_type": "text"
      },
      "source": [
        "Install the packages for tensorflow, keras, torch, sklearn and dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_SMECuUxYqX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "decf49f8-5e51-4128-f6d4-b762cbf46644"
      },
      "source": [
        "import sys\n",
        "import csv\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import json\n",
        "from collections import Counter\n",
        "import nltk\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import warnings\n",
        "import itertools\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "import torch\n",
        "import tensorflow\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D\n",
        "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "\n",
        "#import os\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2,3\"\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy.stats import randint as sp_randint\n",
        "\n",
        "import sklearn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "#warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "#config = tensorflow.ConfigProto( device_count = {'GPU': 3 , 'CPU': 3} ) \n",
        "#sess = tensorflow.Session(config=config) \n",
        "#keras.backend.set_session(sess)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DggGor1Aywv",
        "colab_type": "text"
      },
      "source": [
        "Install the transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEMj9DJ76g0m",
        "colab_type": "code",
        "outputId": "00370a24-e62b-4209-c0be-c877cab290fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install pytorch-pretrained-bert\n",
        "!pip install simpletransformers\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "from simpletransformers.classification import ClassificationModel"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.11.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.17.5)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.10 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.14.10)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.10->boto3->pytorch-pretrained-bert) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.10->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.15.0,>=1.14.10->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "Requirement already satisfied: simpletransformers in /usr/local/lib/python3.6/dist-packages (0.19.7)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.22.1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.17.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (4.28.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.25.3)\n",
            "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2.21.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->simpletransformers) (0.14.1)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->simpletransformers) (2.2.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->simpletransformers) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->simpletransformers) (2.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardx->simpletransformers) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardx->simpletransformers) (3.10.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers->simpletransformers) (0.1.85)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers->simpletransformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers->simpletransformers) (0.0.38)\n",
            "Requirement already satisfied: tokenizers==0.0.11 in /usr/local/lib/python3.6/dist-packages (from transformers->simpletransformers) (0.0.11)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers->simpletransformers) (1.11.10)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (1.24.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardx->simpletransformers) (45.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers->simpletransformers) (7.0)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.10 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers->simpletransformers) (1.14.10)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers->simpletransformers) (0.3.2)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers->simpletransformers) (0.9.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.10->boto3->transformers->simpletransformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u55FfzFIf0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVz5grOgIZ-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rH79WmYA6st",
        "colab_type": "text"
      },
      "source": [
        "Check if gpu works fine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-XfNl7s4mux",
        "colab_type": "code",
        "outputId": "19d35426-c58c-4f96-b736-9901afe26d40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "0.180292369999961\n",
            "GPU (s):\n",
            "0.1798919330001354\n",
            "GPU speedup over CPU: 1x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeaCO23u5zZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Birl7oiFBFry",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing for French Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWYelG-9xg-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_str(string):\n",
        "    string=string.replace(',',' ')\n",
        "    string=string.replace('!',' ')\n",
        "    string=string.replace('.',' ')\n",
        "    string=string.replace('\\'',' ')\n",
        "    string = re.sub(r\"[^A-Za-z(),!?\\'\\`èéêëôòóœàáâç]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" \", string)\n",
        "    string = re.sub(r\"!\", \" \", string)\n",
        "    string = re.sub(r\"\\(\", \" \", string)\n",
        "    string = re.sub(r\"\\)\", \" \", string)\n",
        "    string = re.sub(r\"\\?\", \" \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    string=string.replace('é','e')\n",
        "    string=string.replace('è','e')\n",
        "    string=string.replace('ê','e')\n",
        "    string=string.replace('ç','c')\n",
        "    string=string.replace('ć','c')\n",
        "    string=string.replace('č','c')\n",
        "    string=string.replace('ö','o')\n",
        "    string=string.replace('ô','o')\n",
        "    string=string.replace('ò','o')\n",
        "    string=string.replace('ó','o')\n",
        "    string=string.replace('á','a')\n",
        "    string=string.replace('á','a')\n",
        "    string=string.replace('â','a')\n",
        "    newstring=[]\n",
        "    for a in string.split():\n",
        "        if len(a)>2:\n",
        "            #newstring.append(a)\n",
        "            newstring.append(nltk.stem.WordNetLemmatizer().lemmatize(a))\n",
        "    string=' '.join(newstring)\n",
        "    return string.strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cooF7mz_-MLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kMPxGreBREU",
        "colab_type": "text"
      },
      "source": [
        "Load package data for cnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pb65yl-9x02N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data_and_labels_cnn(filename):\n",
        "    df = pd.read_csv(filename,error_bad_lines=False,na_values=\" \").fillna('nan')\n",
        "    data = df[['polarity','statutnull']]\n",
        "    # random_subset = data.sample(n=5000)\n",
        "    # print(random_subset.head())\n",
        "    # random_subset.to_csv('example.csv')\n",
        "    data['sentiment']=['pos' if (x=='4') else 'neg' for x in data['polarity']]\n",
        "    data['statutnull']= [x.lower() for x in data['statutnull']]\n",
        "    data['statutnull'] = data['statutnull'].apply((lambda x: re.sub('[^A-Za-z(),!?\\'\\`èéêëôòóœàáâç]',' ',x)))\n",
        "    pd.set_option('display.max_colwidth',-1)\n",
        "    data[:5]\n",
        "    titles=data['statutnull'].values\n",
        "    x_text = [clean_str(sent) for sent in titles]\n",
        "    x_text = [s.split(\" \") for s in x_text]\n",
        "    y_input1=pd.get_dummies(data['sentiment']).values\n",
        "    y_input=y_input1\n",
        "    xnew=[]\n",
        "    ynew=[]\n",
        "    for n,a in enumerate(x_text):\n",
        "        if len(a)>4 and len(a)<100:\n",
        "            xnew.append(a)\n",
        "            ynew.append(y_input[n])\n",
        "    return [xnew, ynew]\n",
        "  \n",
        "def pad_sentences_cnn(sentences, padding_word=\"<PAD/>\"):\n",
        "    sequence_length = max(len(x) for x in sentences)\n",
        "    padded_sentences = []\n",
        "    for i in range(len(sentences)):\n",
        "        sentence = sentences[i]\n",
        "        num_padding = sequence_length - len(sentence)\n",
        "        new_sentence = sentence + [padding_word] * num_padding\n",
        "        padded_sentences.append(new_sentence)\n",
        "    return padded_sentences\n",
        "\n",
        "def build_vocab_cnn(sentences):\n",
        "    word_counts = Counter(itertools.chain(*sentences))\n",
        "    # Mapping from index to word\n",
        "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
        "    vocabulary_inv = list(sorted(vocabulary_inv))\n",
        "    # Mapping from word to index\n",
        "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
        "    return [vocabulary, vocabulary_inv]\n",
        "\n",
        "def build_input_data_cnn(sentences,labels, vocabulary):\n",
        "\n",
        "    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
        "    y = np.array(labels)\n",
        "    return [x,y]\n",
        "\n",
        "def load_data_cnn(filename):\n",
        "\n",
        "    # Load and preprocess data\n",
        "    sentences,labels = load_data_and_labels_cnn(filename)\n",
        "    sentences_padded = pad_sentences_cnn(sentences)\n",
        "    vocabulary, vocabulary_inv = build_vocab_cnn(sentences_padded)\n",
        "    x,y = build_input_data_cnn(sentences_padded,labels, vocabulary)\n",
        "    return [x,y, vocabulary, vocabulary_inv]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCYPLAKSCO7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAii3XccBvyt",
        "colab_type": "text"
      },
      "source": [
        "Load the data for BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqQnFnfbBzoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data_and_labels_BERT(filename):\n",
        "    df = pd.read_csv(filename,error_bad_lines=False,na_values=\" \").fillna('nan')\n",
        "    data = df[['polarity','statutnull']]\n",
        "    # random_subset = data.sample(n=5000)\n",
        "    # print(random_subset.head())\n",
        "    # random_subset.to_csv('example.csv')\n",
        "    data['sentiment']=['pos' if (x=='4') else 'neg' for x in data['polarity']]\n",
        "    data['statutnull']= [x.lower() for x in data['statutnull']]\n",
        "    data['statutnull'] = data['statutnull'].apply((lambda x: re.sub('[^A-Za-z(),!?\\'\\`èéêëôòóœàáâç]',' ',x)))\n",
        "    pd.set_option('display.max_colwidth',-1)\n",
        "    data[:5]\n",
        "    titles=data['statutnull'].values\n",
        "    x_text = [clean_str(sent) for sent in titles]\n",
        "    x_text = [s.split(\" \") for s in x_text]\n",
        "    le = LabelEncoder()\n",
        "    y_input1=le.fit_transform(data['sentiment'].values)\n",
        "    #y_input1=pd.get_dummies(data['sentiment']).values\n",
        "    y_input=y_input1\n",
        "    xnew=[]\n",
        "    ynew=[]\n",
        "    for n,a in enumerate(x_text):\n",
        "        if len(a)>4 and len(a)<100:\n",
        "            xnew.append(' '.join(a))\n",
        "            ynew.append(y_input[n])\n",
        "    return [xnew, ynew,le]\n",
        "\n",
        "def load_data_BERT(filename):\n",
        "    # Load and preprocess data\n",
        "    sentences,labels,le = load_data_and_labels_BERT(filename)\n",
        "    # sentences_padded = pad_sentences(sentences)\n",
        "    # vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
        "    # x,y = build_input_data(sentences_padded,labels, vocabulary)\n",
        "    return [sentences,labels,le]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anWQkgKhL3rC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCzgrn29L095",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt53ozCULy6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_yQz50Fx6ih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-NIBsTOCR2O",
        "colab_type": "text"
      },
      "source": [
        "Install the google drive to get the sample input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao5_4XJe17dE",
        "colab_type": "code",
        "outputId": "262014ab-7200-44f8-d66b-f932107ac5ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        }
      },
      "source": [
        "!pip install PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.11)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (1.12.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.3)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.4.2)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (4.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sYbLp70CXFA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2377255-bb46-45bc-972f-2d6c2cc985c7"
      },
      "source": [
        "print('Loading data')\n",
        "downloaded = drive.CreateFile({'id':\"1IyxGimLEytKoAIkbr0wdl_bTdGCuz7uH\"}) \n",
        "downloaded.GetContentFile('example.csv')\n",
        "path2input='example.csv'"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryRMkyoZCeGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bYphn6WCcSF",
        "colab_type": "text"
      },
      "source": [
        "Get the input for CNN and run the example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-F9zIe1x9E5",
        "colab_type": "code",
        "outputId": "03b5a672-b343-454b-9534-82ae3cfd641b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print (path2input)\n",
        "X,Y, vocabulary, vocabulary_inv = load_data_cnn(path2input)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "example.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaFLjRaqDLTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhmhCIa53vkO",
        "colab_type": "code",
        "outputId": "0c31d899-4108-4be1-fe26-da17ad902a78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sequence_length = X.shape[1]\n",
        "vocabulary_size = len(vocabulary_inv)\n",
        "embedding_dim = 300\n",
        "filter_sizes = [1,2,3,4,5,6]\n",
        "num_filters = 512\n",
        "drop = 0.5\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 30\n",
        "\n",
        "print(\"Creating Model...\")\n",
        "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
        "embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
        "reshape = Reshape((sequence_length,embedding_dim,1))(embedding)\n",
        "\n",
        "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "conv_3 = Conv2D(num_filters, kernel_size=(filter_sizes[3], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "conv_4 = Conv2D(num_filters, kernel_size=(filter_sizes[4], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "conv_5 = Conv2D(num_filters, kernel_size=(filter_sizes[5], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
        "maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
        "maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
        "maxpool_3 = MaxPool2D(pool_size=(sequence_length - filter_sizes[3] + 1, 1), strides=(1,1), padding='valid')(conv_3)\n",
        "maxpool_4 = MaxPool2D(pool_size=(sequence_length - filter_sizes[4] + 1, 1), strides=(1,1), padding='valid')(conv_4)\n",
        "maxpool_5 = MaxPool2D(pool_size=(sequence_length - filter_sizes[5] + 1, 1), strides=(1,1), padding='valid')(conv_5)\n",
        "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2, maxpool_3, maxpool_4,maxpool_5])\n",
        "flatten = Flatten()(concatenated_tensor)\n",
        "dropout = Dropout(drop)(flatten)\n",
        "output = Dense(2, activation='softmax')(dropout)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=output)\n",
        "\n",
        "checkpoint = ModelCheckpoint('weights.{epoch:03d}-{val_acc:.4f}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
        "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "\n",
        "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"Traning Model...\")\n",
        "model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint], validation_data=(X_test, Y_test))  # starts training\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating Model...\n",
            "Traning Model...\n",
            "Train on 3247 samples, validate on 812 samples\n",
            "Epoch 1/20\n",
            "3247/3247 [==============================] - 4s 1ms/step - loss: 0.6884 - acc: 0.5343 - val_loss: 0.6762 - val_acc: 0.6268\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.62685, saving model to weights.001-0.6268.hdf5\n",
            "Epoch 2/20\n",
            "3247/3247 [==============================] - 2s 762us/step - loss: 0.6396 - acc: 0.6874 - val_loss: 0.6525 - val_acc: 0.6675\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.62685 to 0.66749, saving model to weights.002-0.6675.hdf5\n",
            "Epoch 3/20\n",
            "3247/3247 [==============================] - 2s 738us/step - loss: 0.5877 - acc: 0.7779 - val_loss: 0.6314 - val_acc: 0.6736\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.66749 to 0.67365, saving model to weights.003-0.6736.hdf5\n",
            "Epoch 4/20\n",
            "3247/3247 [==============================] - 2s 730us/step - loss: 0.5293 - acc: 0.8272 - val_loss: 0.6107 - val_acc: 0.6749\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.67365 to 0.67488, saving model to weights.004-0.6749.hdf5\n",
            "Epoch 5/20\n",
            "3247/3247 [==============================] - 2s 745us/step - loss: 0.4601 - acc: 0.8605 - val_loss: 0.5907 - val_acc: 0.6872\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.67488 to 0.68719, saving model to weights.005-0.6872.hdf5\n",
            "Epoch 6/20\n",
            "3247/3247 [==============================] - 2s 739us/step - loss: 0.3801 - acc: 0.8907 - val_loss: 0.5770 - val_acc: 0.6995\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.68719 to 0.69951, saving model to weights.006-0.6995.hdf5\n",
            "Epoch 7/20\n",
            "3247/3247 [==============================] - 2s 736us/step - loss: 0.2970 - acc: 0.9350 - val_loss: 0.5751 - val_acc: 0.7192\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.69951 to 0.71921, saving model to weights.007-0.7192.hdf5\n",
            "Epoch 8/20\n",
            "3247/3247 [==============================] - 2s 742us/step - loss: 0.2268 - acc: 0.9538 - val_loss: 0.5876 - val_acc: 0.7044\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.71921\n",
            "Epoch 9/20\n",
            "3247/3247 [==============================] - 2s 748us/step - loss: 0.1707 - acc: 0.9680 - val_loss: 0.6094 - val_acc: 0.6970\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.71921\n",
            "Epoch 10/20\n",
            "3247/3247 [==============================] - 2s 749us/step - loss: 0.1264 - acc: 0.9843 - val_loss: 0.6408 - val_acc: 0.6847\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.71921\n",
            "Epoch 11/20\n",
            "3247/3247 [==============================] - 2s 733us/step - loss: 0.0942 - acc: 0.9861 - val_loss: 0.6724 - val_acc: 0.6884\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.71921\n",
            "Epoch 12/20\n",
            "3247/3247 [==============================] - 2s 759us/step - loss: 0.0693 - acc: 0.9945 - val_loss: 0.7084 - val_acc: 0.6810\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.71921\n",
            "Epoch 13/20\n",
            "3247/3247 [==============================] - 2s 733us/step - loss: 0.0526 - acc: 0.9969 - val_loss: 0.7460 - val_acc: 0.6736\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.71921\n",
            "Epoch 14/20\n",
            "3247/3247 [==============================] - 2s 735us/step - loss: 0.0403 - acc: 0.9982 - val_loss: 0.7776 - val_acc: 0.6736\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.71921\n",
            "Epoch 15/20\n",
            "3247/3247 [==============================] - 2s 729us/step - loss: 0.0308 - acc: 0.9994 - val_loss: 0.8159 - val_acc: 0.6650\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.71921\n",
            "Epoch 16/20\n",
            "3247/3247 [==============================] - 2s 730us/step - loss: 0.0241 - acc: 0.9997 - val_loss: 0.8544 - val_acc: 0.6650\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.71921\n",
            "Epoch 17/20\n",
            "3247/3247 [==============================] - 2s 730us/step - loss: 0.0190 - acc: 0.9997 - val_loss: 0.8869 - val_acc: 0.6650\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.71921\n",
            "Epoch 18/20\n",
            "3247/3247 [==============================] - 2s 733us/step - loss: 0.0154 - acc: 1.0000 - val_loss: 0.9214 - val_acc: 0.6650\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.71921\n",
            "Epoch 19/20\n",
            "3247/3247 [==============================] - 2s 731us/step - loss: 0.0123 - acc: 1.0000 - val_loss: 0.9523 - val_acc: 0.6601\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.71921\n",
            "Epoch 20/20\n",
            "3247/3247 [==============================] - 2s 717us/step - loss: 0.0102 - acc: 0.9997 - val_loss: 0.9857 - val_acc: 0.6589\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.71921\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7c61c2b828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8drUdxlDCAV",
        "colab_type": "text"
      },
      "source": [
        "Get the data for BERT and run the example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y681iQUDCde",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "05d4d92e-1c52-466b-d07c-5ced61b9b890"
      },
      "source": [
        "print (path2input)\n",
        "X,Y,le= load_data_BERT(path2input)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "example.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD5EXFykRaTS",
        "colab_type": "text"
      },
      "source": [
        "Data and model preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ68jfW7-8OZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.DataFrame({'text':X_train,'labels':Y_train})\n",
        "eval_df = pd.DataFrame({'text':X_test,'labels':Y_test})\n",
        "\n",
        "# Create a ClassificationModel\n",
        "#model = ClassificationModel('distilbert', 'distilbert-base-multilingual-cased', num_labels=2, args={'reprocess_input_data': True, 'overwrite_output_dir': True,'fp16': False,'fp16': False,\"train_batch_size\": 8,\"num_train_epochs\": 5})\n",
        "#model = ClassificationModel('camembert', 'camembert-base', num_labels=2, args={'reprocess_input_data': True, 'overwrite_output_dir': True,'num_train_epochs': 3,'fp16': False,\"train_batch_size\": 8})\n",
        "model = ClassificationModel('bert', 'bert-base-multilingual-uncased', num_labels=2, args={'reprocess_input_data': True, 'overwrite_output_dir': True,'num_train_epochs': 3,'fp16': False,\"train_batch_size\": 8})\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6XxvbZQEE6c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d4faa29405ab4b13a13ec543aa4516b9",
            "5058cba4d26543bca499d2b6a4526378",
            "b2f97daae4c543adbb1aa00f71692def",
            "4d93031e4d0c40559642a8393c6e69f4",
            "6363711a33ad4c51b4247a7310013ba7"
          ]
        },
        "outputId": "68644ffb-d508-48cc-e4a8-b6187b5b849f"
      },
      "source": [
        "# Train the model\n",
        "model.train_model(train_df,eval_df)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converting to features started. Cache is not used.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4faa29405ab4b13a13ec543aa4516b9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=3247), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5058cba4d26543bca499d2b6a4526378",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=3, style=ProgressStyle(description_width='initial…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2f97daae4c543adbb1aa00f71692def",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Current iteration', max=406, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Running loss: 0.674116"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Running loss: 0.784267"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d93031e4d0c40559642a8393c6e69f4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Current iteration', max=406, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Running loss: 0.650750"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6363711a33ad4c51b4247a7310013ba7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Current iteration', max=406, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Running loss: 0.425696Training of bert model complete. Saved to outputs/.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFpFxX2pRLyE",
        "colab_type": "text"
      },
      "source": [
        "The result for MBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJIyIjS0Pu1j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51,
          "referenced_widgets": [
            "997013b0fcfd4b76a78b97374494f4fb",
            "091c6b8fd2ba46a58f5d5b75caf522ec"
          ]
        },
        "outputId": "dcf1cf51-6fa2-493f-bbfd-08ff0665b3d9"
      },
      "source": [
        "result, model_outputs, wrong_predictions = model.eval_model(eval_df, acc=sklearn.metrics.accuracy_score)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converting to features started. Cache is not used.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "997013b0fcfd4b76a78b97374494f4fb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=812), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "091c6b8fd2ba46a58f5d5b75caf522ec",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=102), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'mcc': 0.4670797631240588, 'tp': 272, 'tn': 324, 'fp': 96, 'fn': 120, 'acc': 0.7339901477832512, 'eval_loss': 0.5925741793186057}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}